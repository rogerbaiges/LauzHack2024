import openai

class LLM:
	def __init__(self, api_key: str, model: str = "gpt-4o-latest", temperature: float = 0.7, 
				 max_tokens: int = 100, system_prompt: str | None = None) -> None:
		"""
		Initializes the LLM class with configuration for OpenAI API.

		Args:
			api_key (str): OpenAI API key.
			model (str): The model to use (default is "gpt-4-latest").
			temperature (float): Controls the creativity of the responses.
			max_tokens (int): Maximum number of tokens for the generated response.
			system_prompt (str): A prompt to define the LLM's behavior.
		"""
		assert api_key, "API key must be provided"

		openai.api_key = api_key
		self.model = model
		self.temperature = temperature
		self.max_tokens = max_tokens
		self.history = []  # Conversation history

		# Set initial prompt in history if provided
		if system_prompt:
			self.add_message_to_history("system", system_prompt)

	def add_message_to_history(self, role: str, content: str) -> None:
		"""
		Adds a message to the conversation history.

		Args:
			role (str): The role of the message ("system", "user", or "assistant").
			content (str): The content of the message.
		"""
		self.history.append({"role": role, "content": content})

	def ask(self, prompt: str, return_only_content: bool = True) -> str:
		"""
		Sends a prompt to the model and retrieves the response.

		Args:
			prompt (str): The user input to send to the model.
			return_only_content (bool): Whether to return only the content of the response.

		Returns:
			str: The response generated by the model.
		"""
		self.add_message_to_history("user", prompt)
		messages = self.history

		try:
			# API call to OpenAI
			response = openai.chat.completions.create(
				model=self.model,
				messages=messages,
				temperature=self.temperature,
				max_tokens=self.max_tokens,
			)

			# Extract the assistant's response
			assistant_message = response.choices[0].message.content if response.choices else ""

			# Add the response to history if history is included
			self.add_message_to_history("assistant", assistant_message)

			if return_only_content:
				return assistant_message
			else:
				return response

		
		except Exception as e:
			return f"Error connecting to the model: {e}"

	def clear_history(self) -> None:
		"""
		Clears the conversation history.
		"""
		self.history = []

	def configure(self, temperature: float | None = None, max_tokens: int | None = None) -> None:
		"""
		Configures the model parameters.

		Args:
			temperature (float): Controls the creativity of the responses.
			max_tokens (int): Maximum number of tokens for the generated response.
		"""
		if temperature is not None:
			self.temperature = temperature
		if max_tokens is not None:
			self.max_tokens = max_tokens
